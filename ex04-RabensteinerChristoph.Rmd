---
title: "Übung 04"
author: "Algorithmik und Statistik 2 LAB, SS2019"
date: 'Bis: Sonntag, 23. Juni 2019, 23:59 Uhr'
urlcolor: cyan
---

```{r options, include = FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

Bitte um Beachtung der [Übungs-Policy](https://weblearn.fh-kufstein.ac.at/mod/page/view.php?id=46374) für genaue Anweisungen und einige Beurteilungsnotizen. Fehler bei der Einhaltung ergeben Punktabzug.


## Aufgabe 1

**[10 points]** Für diese Frage verwenden wir die `OJ`Daten aus dem `ISLR`-Paket. Wir werden versuchen, die Variable "Purchase" vorherzusagen. Nachdem Sie `uin` zu Ihrem `UIN` geändert haben, verwenden Sie den folgenden Code, um die Daten aufzuteilen.

```{r, message = FALSE, warning = FALSE}
library(ISLR)
library(caret)
library(MLmetrics)
uin = 1810837995
set.seed(uin)
oj_idx = createDataPartition(OJ$Purchase, p = 0.5, list = FALSE)
oj_trn = OJ[oj_idx,]
oj_tst = OJ[-oj_idx,]
```

**(a)** Stimmen Sie ein SVM mit linearem Kernel mit 5-facher Cross-Validierung auf die Trainingsdaten ab. Verwenden Sie das folgende Wertgitter für `C`. Berichten Sie die gewählten Werte aller Tuningparameter + Testgenauigkeit.

Die Tabelle besteht aus 18 Features. Die Variable `Purchase` gilt es mit den 17 übrigen Prädiktoren zu bestimmen. Die Variable `Purchase`ist ein Factor mit zwei verschiedenen Werten: deshalb verwenden wir hier Klassifikationsmodelle für unsere Vorhersagen.
Struktur der Tabelle:
```{r, echo=FALSE}
str(OJ)
```

Wertgitter für `C`
```{r, echo=T}
lin_grid = expand.grid(C = c(2 ^ (-5:5)))
```

5-fache Kreuzvalidierung:
```{r, echo=T}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Modeltraining mit obenbeschriebenen Wertgitter für 'C'
```{r, echo=T}
svm.tune1 <- train(Purchase~.,data=oj_trn,
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    tuneGrid = lin_grid,
                    metric="ROC",
                    trControl=ctrl)	

PredictTest1 = predict(svm.tune1,oj_tst)
```

Das oben errechnete Model hat eine Genauigkeit von 83,52%. 
Ausgabe der Confusion Matrix:
```{r, echo=F}
svm1 <- confusionMatrix(PredictTest1,oj_tst$Purchase)
svm1
```

Ausgabe der Tuningsparameter:
```{r, echo=F}
#Tuningparameter
svm.tune1
```

**Off Topic** Da der Tuningsparameter auf den Rand geht, erweitern wir den Wertgitter für `C` nach unten. 
Wertgitter für `C`
```{r, echo=T}
lin_grid2 = expand.grid(C = c(2 ^ (-10:1)))
```

Modelltraining und Testen mit dem geänderten Wertegitter:
```{r, echo=T}
svm.tune2 <- train(Purchase~.,data=oj_trn,
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    tuneGrid = lin_grid2,
                    metric="ROC",
                    trControl=ctrl)	

PredictTest2 = predict(svm.tune2,oj_tst)
```
Das oben errechnete Model hat eine Genauigkeit von 83,15%. 
Ausgabe der Confusion Matrix:
```{r}
svm2 <- confusionMatrix(PredictTest2,oj_tst$Purchase)
svm2
```
Beim Ändern des Wertgitters für `C`ändert sich C von 0.03125 auf 0.0625. 
Die Genauigkeit des Modells konnte nicht mit dem Ändern des Wertgitters nicht verbessert werden.
Tuningsparameter:
```{r}
svm.tune2
```

**(b)** Abstimmung eines SVM mit Polynomkern auf die Trainingsdaten mittels 5-facher Cross-Validierung. Geben Sie kein Tuning-Grid an. (`caret` wird einen für Sie erstellen.) Berichten Sie die gewählten Werte aller Tuning-Parameter. Berichten Sie über die Genauigkeit der Testdaten.

5-fache Kreuzvalidierung
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit SVM mit Polynomkern
```{r}
svm.tune3 <- train(Purchase~.,data=oj_trn,
                    method = "svmPoly",
                    preProc = c("center","scale"),
                    metric="ROC",
                    trControl=ctrl)	
```

Modeltest:
```{r}
PredictTest3 = predict(svm.tune3,oj_tst)
```

Confusion Matrix:
```{r, echo=F}
svm3 <- confusionMatrix(PredictTest3,oj_tst$Purchase)
svm3
```

Ausgabe der Tuningparameter:
```{r, echo = F}
svm.tune3
```



**(c)** Stimmen Sie ein SVM mit Radialkernel mit 5-facher Cross-Validierung auf die Trainingsdaten ab. Verwenden Sie das folgende Wertgitter für `C` und `sigma`. Berichten Sie die gewählten Werte aller Tuningparameter. Berichten Sie über die Genauigkeit der Testdaten. 

```{r}
rad_grid = expand.grid(C = c(2 ^ (-2:3)), sigma  = c(2 ^ (-3:1)))
```

5-fache Kreuzvalidierung
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit SVM mit Radialkernel
```{r}
svm.tune4 <- train(Purchase~.,data=oj_trn,
                    method = "svmRadial",
                    preProc = c("center","scale"),
                    tuneGrid = rad_grid,
                    metric="ROC",
                    trControl=ctrl)	
```

Modeltest
```{r}
PredictTest4 = predict(svm.tune4,oj_tst)
```

Confusion Matrix
```{r, echo=F}
svm4 <- confusionMatrix(PredictTest4,oj_tst$Purchase)
svm4
```

Ausgabe der Tuningparameter:
```{r, echo=F}
svm.tune4
```

**(d)** Stimmen Sie einen Random Forest mit einer 5-fachen Kreuzvalidierung ab. Berichten Sie die gewählten Werte aller Tuningparameter. Berichten Sie über die Genauigkeit der Testdaten.

5-fache Kreuzvalidierung:
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit Random Forest
```{r}

rf.tune <- train(Purchase~.,data=oj_trn,
                    method = "rf",
                    preProc = c("center","scale"),
                    metric="ROC",
                    trControl=ctrl)	
```

Testen des Models:
```{r}
PredictTest5 = predict(rf.tune,oj_tst)
```

Confusion Matrix:
```{r,echo=F}
rf1 <- confusionMatrix(PredictTest5,oj_tst$Purchase)
rf1
```

Ausgabe derr Tuningsparameter:
```{r,echo=F}
rf.tune
```

**(e)** Fassen Sie die obigen Genauigkeiten zusammen. Welche Methode hat am besten funktioniert? Warum?

```{r,echo=F}
Genauigkeiten <- data.frame(
  GenauigkeitenName = c('SVM with Linear Kernel','SVM with Polynomial Kernel','SVM with Radial Kernel','Random Forest'), 

  GenauigkeitenTest = c(Accuracy(PredictTest1, oj_tst$Purchase), 
                         Accuracy(PredictTest3, oj_tst$Purchase), 
                         Accuracy(PredictTest4, oj_tst$Purchase),
                         Accuracy(PredictTest5, oj_tst$Purchase)),
  SensitivityTest = c(svm1$byClass['Sensitivity'],
                      svm3$byClass['Sensitivity'],
                      svm4$byClass['Sensitivity'],
                      rf1$byClass['Sensitivity']),
  SpecificityTest = c(svm1$byClass['Specificity'],
                      svm3$byClass['Specificity'],
                      svm4$byClass['Specificity'],
                      rf1$byClass['Specificity']) , 
  PrecisionTest = c(svm1$byClass['Precision'],
                      svm3$byClass['Precision'],
                      svm4$byClass['Precision'],
                      rf1$byClass['Precision'])                   
)
  
colnames(Genauigkeiten) <- c('Model', 'Accuracy (Test)', 'Sensitivity','Specificity','Precision')
svm1$byClass['Sensitivity']
svm1$byClass['Precision']
svm1$byClass
```

Zusammenfassung Genauigkeiten:
```{r, echo=False}
Genauigkeiten
```
Beim Vergleich der obigen Modelle fällt auf, dass die SVM mit Polynomial Kernel am besten abschneidet. Nicht nur die Genauigkeit beim Testen ist am höchsten, sondern auch die `Sensitivity`(Percentage of positive instances out of the total acutal positive instances), `Specificity` (Percentage of negative instances out of the total acutal negative instances) und `Precision`(Percentage of positive instances out of the total predicted positive instances). In den jeweiligen Confusion Matrizen stehen noch weitere Werte zum Vergleich.

# Aufgabe 2

**[10 points]** Verwenden Sie für diese Frage die Daten in `clust_data.csv`. Wir werden versuchen, diese Daten mit $k$-means zu bündeln. Aber, welche $k$ sollen wir verwenden?
```{r}
clust = read.csv('clust_data.csv')
str(clust)
```


**(a)** Wenden Sie $k$-means 15 mal auf diese Daten an, wobei Sie die Anzahl der Zentren von 1 bis 15 verwenden. Verwenden Sie jedes Mal `nstart = 10` und speichern Sie den Wert `tot.withinss` aus dem resultierenden Objekt. (Hinweis: Schreiben Sie eine for-Schleife.) Die `tot.withinss` misst, wie variabel die Beobachtungen innerhalb eines Clusters sind, das wir gerne niedrig halten würden. Offensichtlich wird dieser Wert also mit mehr Zentren niedriger sein, egal wie viele Cluster es wirklich gibt. Zeichne diesen Wert gegen die Anzahl der Zentren auf. Suchen Sie nach einem "Ellenbogen", der Anzahl der Zentren, in denen die Verbesserung plötzlich wegfällt. Basierend auf dieser Darstellung, wie viele Cluster sollten Ihrer Meinung nach für diese Daten verwendet werden?
```{r}
mydata = data.frame(Centers = NA, Withinss=NA)

for (i in 1:15){
clusters <- kmeans(clust, centers=i, nstart=10)
mydata[i,2] <- clusters$tot.withinss
mydata[i,1] <- i
}

plot(mydata)

```


```{r, eval=F}
# Hinweis, wie man Daten clustern könnte - Domainen-Wissen fehlt aber.
# cluster mit 4 center vorteilhaft, also in der Beuge. Nur hinweis auf mögliche Cluster
```

**(b)** Wenden Sie $k$-means für die von Ihnen gewählte Anzahl von Zentren erneut an. Wie viele Beobachtungen werden in jedem Cluster platziert? Was ist der Wert von `tot.withinss`?
```{r}
km.out = kmeans(clust,centers=4,nstart=10)
km.out$withinss
plot(km.out$withinss)
km.out$tot.withinss
```
**(c)** Visualisieren Sie diese Daten. Plotten Sie die Daten mit den ersten beiden Variablen und färben Sie die Punkte entsprechend des $k$-means clusterings. Basierend auf diesem Plot, denken Sie, dass Sie eine gute Wahl für die Anzahl der Zentren getroffen haben? (Kurze Erklärung.)
```{r}
ggplot(data.frame(clust), aes(clust[,1], clust[,2], color = km.out$cluster)) + geom_point()
```
**(d)** Verwenden Sie PCA, um diese Daten zu visualisieren. Plotten Sie die Daten mit den ersten beiden Hauptkomponenten und färben Sie die Punkte entsprechend dem $k$-means Clustering. Basierend auf diesem Plot, denken Sie, dass Sie eine gute Wahl für die Anzahl der Zentren getroffen haben? (Kurze Erklärung.)
```{r}
clust.pca <- prcomp(clust,
                    center = TRUE,
                    scale = TRUE)

plot(clust.pca, type = "l")
predict(clust.pca, newdata = tail(clust,2))

plot(clust.pca$x[,1:2], col = km.out$cluster)

ggplot(data.frame(clust), aes(clust.pca$x[,1], clust.pca$x[,2], color = km.out$cluster)) + geom_point()
```
**(e)** Berechnen Sie den Anteil der Variation, der durch die Hauptkomponenten erklärt wird. Machen Sie eine Darstellung des kumulierten Anteils erklärt. Wie viele Hauptkomponenten sind notwendig, um 95% der Variation der Daten zu erklären?
```{r}
summary(clust.pca)
```



# Aufgabe 3

**[10 points]** Für diese Frage werden wir auf die `USArrests` Daten aus den Notizen zurückkommen. (Dies ist ein Standarddatensatz von `R`.)
```{r}
USArrests
```
**(a)** Führen Sie hierarchisches Clustering sechsmal durch. Berücksichtigen Sie alle möglichen Kombinationen von Verknüpfungen (Average, Single, Complete) und Datenskalierung. (Skaliert, Nicht skaliert.)

| Linkage  | Scaling |
|----------|---------|
| Single   | No      |
| Average  | No      |
| Complete | No      |
| Single   | Yes     |
| Average  | Yes     |
| Complete | Yes     |

Schneiden Sie das Dendrogramm jedes Mal auf eine Höhe, die zu vier verschiedenen Clustern führt. Plotten Sie die Ergebnisse mit einer Farbe für jeden Cluster.
```{r}
x = USArrests
#standardize <- function(x){(x-min(x))/(max(x)-min(x))}
scale(x)
str(x)

dist_mat <- dist(x, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')

cut_avg <- cutree(hclust_avg, k = 4)
plot(hclust_avg)
rect.hclust(hclust_avg, k=4)
abline(h = 3, col = 'red')
```
**(b)** Basierend auf den obigen Plots, erscheint eines der Ergebnisse nützlicher als die anderen? (Es gibt hier keine richtige Antwort.) Wählen Sie Ihren Favoriten. (Nochmals, keine richtige Antwort.)
```{r}

```
**(c)** Verwenden Sie die Dokumentation zu `?hclust`, um weitere mögliche Verknüpfungen zu finden. Such dir einen aus und probiere ihn aus. Vergleichen Sie die Ergebnisse mit Ihren Favoriten von **(b)**. Ist es anders?
```{r}

```
**(d)** Verwenden Sie die Dokumentation zu `?dist`, um andere mögliche Entfernungsmessungen zu finden. (Wir haben `euklidisch` verwendet.) Wählen Sie eine (nicht `binär`) und versuchen Sie es. Vergleichen Sie die Ergebnisse mit Ihren Favoriten von **(b)**. Ist es anders?
```{r}

```
