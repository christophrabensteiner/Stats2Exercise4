---
title: "Übung 04"
author: "Algorithmik und Statistik 2 LAB, SS2019"
date: 'Bis: Sonntag, 23. Juni 2019, 23:59 Uhr'
output:
  html_document:
    df_print: paged
urlcolor: cyan
---

```{r options, include = FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

Bitte um Beachtung der [Übungs-Policy](https://weblearn.fh-kufstein.ac.at/mod/page/view.php?id=46374) für genaue Anweisungen und einige Beurteilungsnotizen. Fehler bei der Einhaltung ergeben Punktabzug.


## Aufgabe 1

**[10 points]** Für diese Frage verwenden wir die `OJ`Daten aus dem `ISLR`-Paket. Wir werden versuchen, die Variable "Purchase" vorherzusagen. Nachdem Sie `uin` zu Ihrem `UIN` geändert haben, verwenden Sie den folgenden Code, um die Daten aufzuteilen.

```{r, echo=F, message = FALSE, warning = FALSE}
library(ISLR)
library(caret)
library(MLmetrics)
library(stats)
library(cluster)
library(dendextend)
library(randomForest)
```

```{r}
uin = 1810837995
set.seed(uin)
oj_idx = createDataPartition(OJ$Purchase, p = 0.5, list = FALSE)
oj_trn = OJ[oj_idx,]
oj_tst = OJ[-oj_idx,]
```

**(a)** Stimmen Sie ein SVM mit linearem Kernel mit 5-facher Cross-Validierung auf die Trainingsdaten ab. Verwenden Sie das folgende Wertgitter für `C`. Berichten Sie die gewählten Werte aller Tuningparameter + Testgenauigkeit.

Die Tabelle besteht aus 18 Features. Die Variable `Purchase` gilt es mit den 17 übrigen Prädiktoren zu bestimmen. Die Variable `Purchase`ist ein Factor mit zwei verschiedenen Werten: deshalb verwenden wir hier Klassifikationsmodelle für unsere Vorhersagen.


Struktur der Tabelle:
```{r, echo=FALSE}
str(OJ)
```

Wertgitter für `C`
```{r, echo=T}
lin_grid = expand.grid(C = c(2 ^ (-5:5)))
```

5-fache Kreuzvalidierung:
```{r, echo=T}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Modeltraining mit obenbeschriebenen Wertgitter für 'C'
```{r, echo=T}
svm.tune1 <- train(Purchase~.,data=oj_trn,
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    tuneGrid = lin_grid,
                    metric="ROC",
                    importance = T,
                    trControl=ctrl)	
```
Testen des Models:
```{r}
PredictTest1 = predict(svm.tune1,oj_tst)
```

Das oben errechnete Model hat eine Genauigkeit von über 83%. 
Ausgabe der Confusion Matrix:
```{r, echo=F}
svm1 <- confusionMatrix(PredictTest1,oj_tst$Purchase)
svm1
```


Ausgabe der Tuningsparameter:
```{r, echo=F}
#Tuningparameter
svm.tune1
```

**Off Topic** Da der Tuningsparameter auf den Rand geht, erweitern wir den Wertgitter für `C` nach unten. 
Wertgitter für `C`
```{r, echo=T}
lin_grid2 = expand.grid(C = c(2 ^ (-10:1)))
```

Modelltraining und Testen mit dem geänderten Wertegitter:
```{r, echo=T}
svm.tune2 <- train(Purchase~.,data=oj_trn,
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    tuneGrid = lin_grid2,
                    metric="ROC",
                    trControl=ctrl)	
```

Testen des Models:
```{r}
PredictTest2 = predict(svm.tune2,oj_tst)
```
Das oben errechnete Model hat eine Genauigkeit von 83,15%. 
Ausgabe der Confusion Matrix:
```{r}
svm2 <- confusionMatrix(PredictTest2,oj_tst$Purchase)
svm2
```
Beim Ändern des Wertgitters für `C`ändert sich C von 0.03125 auf 0.03125. 
Die Genauigkeit des Modells konnte nicht mit dem Ändern des Wertgitters nicht verbessert werden.
Tuningsparameter:
```{r}
svm.tune2
```

**(b)** Abstimmung eines SVM mit Polynomkern auf die Trainingsdaten mittels 5-facher Cross-Validierung. Geben Sie kein Tuning-Grid an. (`caret` wird einen für Sie erstellen.) Berichten Sie die gewählten Werte aller Tuning-Parameter. Berichten Sie über die Genauigkeit der Testdaten.

5-fache Kreuzvalidierung:
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit SVM mit Polynomkern:
```{r}
svm.tune3 <- train(Purchase~.,data=oj_trn,
                    method = "svmPoly",
                    preProc = c("center","scale"),
                    metric="ROC",
                    trControl=ctrl)	
```

Testen des Models:
```{r}
PredictTest3 = predict(svm.tune3,oj_tst)
```

Confusion Matrix:
```{r, echo=F}
svm3 <- confusionMatrix(PredictTest3,oj_tst$Purchase)
svm3
```

Ausgabe der Tuningparameter:
```{r, echo = F}
svm.tune3
```



**(c)** Stimmen Sie ein SVM mit Radialkernel mit 5-facher Cross-Validierung auf die Trainingsdaten ab. Verwenden Sie das folgende Wertgitter für `C` und `sigma`. Berichten Sie die gewählten Werte aller Tuningparameter. Berichten Sie über die Genauigkeit der Testdaten. 

```{r}
rad_grid = expand.grid(C = c(2 ^ (-2:3)), sigma  = c(2 ^ (-3:1)))
```

5-fache Kreuzvalidierung:
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit SVM mit Radialkernel:
```{r}
svm.tune4 <- train(Purchase~.,data=oj_trn,
                    method = "svmRadial",
                    preProc = c("center","scale"),
                    tuneGrid = rad_grid,
                    metric="ROC",
                    trControl=ctrl)	
```

Testen des Models:
```{r}
PredictTest4 = predict(svm.tune4,oj_tst)
```

Confusion Matrix:
```{r, echo=F}
svm4 <- confusionMatrix(PredictTest4,oj_tst$Purchase)
svm4
```

Ausgabe der Tuningparameter:
```{r, echo=F}
svm.tune4
```

**(d)** Stimmen Sie einen Random Forest mit einer 5-fachen Kreuzvalidierung ab. Berichten Sie die gewählten Werte aller Tuningparameter. Berichten Sie über die Genauigkeit der Testdaten.

5-fache Kreuzvalidierung:
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats=5,	
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)
```

Training mit Random Forest:
```{r}
rf.tune <- train(Purchase~.,data=oj_trn,
                    method = "rf",
                    preProc = c("center","scale"),
                    metric="ROC",
                    importance = T,
                    trControl=ctrl)	

```

Die wichtigsten Variablen für Random Forest sind folgende:
```{r}
varImp(rf.tune)
```

Testen des Models:
```{r}
PredictTest5 = predict(rf.tune,oj_tst)
```

Confusion Matrix:
```{r,echo=F}
rf1 <- confusionMatrix(PredictTest5,oj_tst$Purchase)
rf1
```

Ausgabe derr Tuningsparameter:
```{r,echo=F}
rf.tune
```

**(e)** Fassen Sie die obigen Genauigkeiten zusammen. Welche Methode hat am besten funktioniert? Warum?

```{r,echo=F}
Genauigkeiten <- data.frame(
  GenauigkeitenName = c('SVM with Linear Kernel','SVM with Polynomial Kernel','SVM with Radial Kernel','Random Forest'), 

  GenauigkeitenTest = c(Accuracy(PredictTest1, oj_tst$Purchase), 
                         Accuracy(PredictTest3, oj_tst$Purchase), 
                         Accuracy(PredictTest4, oj_tst$Purchase),
                         Accuracy(PredictTest5, oj_tst$Purchase)),
  SensitivityTest = c(svm1$byClass['Sensitivity'],
                      svm3$byClass['Sensitivity'],
                      svm4$byClass['Sensitivity'],
                      rf1$byClass['Sensitivity']),
  SpecificityTest = c(svm1$byClass['Specificity'],
                      svm3$byClass['Specificity'],
                      svm4$byClass['Specificity'],
                      rf1$byClass['Specificity']) , 
  PrecisionTest = c(svm1$byClass['Precision'],
                      svm3$byClass['Precision'],
                      svm4$byClass['Precision'],
                      rf1$byClass['Precision'])                   
)
  
colnames(Genauigkeiten) <- c('Model', 'Accuracy (Test)', 'Sensitivity','Specificity','Precision')
```

Zusammenfassung der Genauigkeiten:
```{r, echo=FALSE}
Genauigkeiten
```
Beim Vergleich der obigen Modelle fällt auf, dass die SVM mit Polynomial Kernel am besten abschneidet. Nicht nur die Genauigkeit beim Testen ist am höchsten, sondern auch die `Sensitivity`(Percentage of positive instances out of the total acutal positive instances), `Specificity` (Percentage of negative instances out of the total acutal negative instances) und `Precision`(Percentage of positive instances out of the total predicted positive instances). In den jeweiligen Confusion Matrizen stehen noch weitere Werte zum Vergleich.

# Aufgabe 2

**[10 points]** Verwenden Sie für diese Frage die Daten in `clust_data.csv`. Wir werden versuchen, diese Daten mit $k$-means zu bündeln. Aber, welche $k$ sollen wir verwenden?
```{r, results="hide"}
clust = read.csv('clust_data.csv')
scale(clust)
```


**(a)** Wenden Sie $k$-means 15 mal auf diese Daten an, wobei Sie die Anzahl der Zentren von 1 bis 15 verwenden. Verwenden Sie jedes Mal `nstart = 10` und speichern Sie den Wert `tot.withinss` aus dem resultierenden Objekt. (Hinweis: Schreiben Sie eine for-Schleife.) Die `tot.withinss` misst, wie variabel die Beobachtungen innerhalb eines Clusters sind, das wir gerne niedrig halten würden. Offensichtlich wird dieser Wert also mit mehr Zentren niedriger sein, egal wie viele Cluster es wirklich gibt. Zeichne diesen Wert gegen die Anzahl der Zentren auf. Suchen Sie nach einem "Ellenbogen", der Anzahl der Zentren, in denen die Verbesserung plötzlich wegfällt. Basierend auf dieser Darstellung, wie viele Cluster sollten Ihrer Meinung nach für diese Daten verwendet werden?
```{r}
mydata = data.frame(Centers = NA, Withinss=NA)

for (i in 1:15){
clusters <- kmeans(clust, centers=i, nstart=10)
mydata[i,2] <- clusters$tot.withinss
mydata[i,1] <- i
}

plot(mydata, main='Clustering - Wert von tot.withniss nach den Centers')
```



**(b)** Wenden Sie $k$-means für die von Ihnen gewählte Anzahl von Zentren erneut an. Wie viele Beobachtungen werden in jedem Cluster platziert? Was ist der Wert von `tot.withinss`?
```{r, echo=T}
km.out = kmeans(clust,centers=4,nstart=10)
```

Wert von `tot.withinss`
```{r, echo=F}
km.out$tot.withinss
```

**(c)** Visualisieren Sie diese Daten. Plotten Sie die Daten mit den ersten beiden Variablen und färben Sie die Punkte entsprechend des $k$-means clusterings. Basierend auf diesem Plot, denken Sie, dass Sie eine gute Wahl für die Anzahl der Zentren getroffen haben? (Kurze Erklärung.)
```{r,echo=F}
plot1 <-ggplot(data.frame(clust), aes(clust[,1], clust[,2], color = km.out$cluster)) + geom_point()
plot1 + scale_color_continuous(low='red', high = 'green')
```
Die Zentren sind mit 4 am eindeutigsten. Bei höheren/mehreren Zentren gibt es keine allzugroße Verbesserung. 

**(d)** Verwenden Sie PCA, um diese Daten zu visualisieren. Plotten Sie die Daten mit den ersten beiden Hauptkomponenten und färben Sie die Punkte entsprechend dem $k$-means Clustering. Basierend auf diesem Plot, denken Sie, dass Sie eine gute Wahl für die Anzahl der Zentren getroffen haben? (Kurze Erklärung.)

Berechung PCA:
```{r, results="hide"}
clust.pca <- prcomp(clust,
                    center = T,
                    scale = T)

```
 
Variances:
```{r, echo=F}
plot(clust.pca, type = "l", main='Variances')
```

```{r, results="hide"}
predict(clust.pca, newdata = tail(clust,2))
```

Ausgabe PCA-Plot:
```{r,echo=FALSE}
#plot(clust.pca$x[,1:2], col = km.out$cluster)
ggplot(data.frame(clust), aes(clust.pca$x[,1], clust.pca$x[,2], color = km.out$cluster)) + geom_point()
```
Die Zahl der Zentren (=4) ist hier gut getroffen, da die Daten sehr gut klassifiziert werden können.

**(e)** Berechnen Sie den Anteil der Variation, der durch die Hauptkomponenten erklärt wird. Machen Sie eine Darstellung des kumulierten Anteils erklärt. Wie viele Hauptkomponenten sind notwendig, um 95% der Variation der Daten zu erklären?
```{r, echo=F}
summary(clust.pca)
```
Die ersten vier PC erklären nur 40%. Um 95% der Hauptkomponenten erklären zu können, braucht es insgesamt 37 PC.

# Aufgabe 3

**[10 points]** Für diese Frage werden wir auf die `USArrests` Daten aus den Notizen zurückkommen. (Dies ist ein Standarddatensatz von `R`.)
```{r, echo=FALSE}
str(USArrests)
```
**(a)** Führen Sie hierarchisches Clustering sechsmal durch. Berücksichtigen Sie alle möglichen Kombinationen von Verknüpfungen (Average, Single, Complete) und Datenskalierung. (Skaliert, Nicht skaliert.)

| Linkage  | Scaling |
|----------|---------|
| Single   | No      |
| Average  | No      |
| Complete | No      |
| Single   | Yes     |
| Average  | Yes     |
| Complete | Yes     |


Schneiden Sie das Dendrogramm jedes Mal auf eine Höhe, die zu vier verschiedenen Clustern führt. Plotten Sie die Ergebnisse mit einer Farbe für jeden Cluster.

**unskalierte Daten**
Distanzmatrix: Euklidische Distanz // unskalierte Daten
```{r}
df1 <- na.omit(USArrests)
dist_mat <- dist(df1, method = 'euclidean')
```

*1)* Linkage: Single | Scaling: No
```{r}
hc_single_scaling_no <- hclust(dist_mat, method = 'single')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```

Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
single_dend_obj <- as.dendrogram(hc_single_scaling_no)
single_col_dend <- color_branches(single_dend_obj, k = 4)
plot(single_col_dend, main="Hierarchisches Clustern: Single-Linkage und unskaliert ")
```

*2)* Linkage: Average | Scaling: No
```{r}
hc_average_scaling_no <- hclust(dist_mat, method = 'average')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
average_dend_obj <- as.dendrogram(hc_average_scaling_no)
average_col_dend <- color_branches(average_dend_obj, k = 4)
plot(average_col_dend,main="Hierarchisches Clustern: Average-Linkage und unskaliert ")
```

*3)* Linkage: Complete | Scaling: No
```{r}
hc_complete_scaling_no <- hclust(dist_mat, method = 'complete')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
complete_dend_obj <- as.dendrogram(hc_complete_scaling_no)
complete_col_dend <- color_branches(complete_dend_obj, k = 4)
plot(complete_col_dend,main="Hierarchisches Clustern: Complete-Linkage und unskaliert ")
```

**skalierte Daten**

Distanzmatrix: Euklidische Distanz // skalierte Daten
```{r}
df2 <- scale(na.omit(USArrests))
dist_mat2 <- dist(df2, method = 'euclidean')
```

*4)* Linkage: Single | Scaling: Yes
```{r}
hc_single_scaling_yes <- hclust(dist_mat2, method = 'single')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
single_dend_obj_scaled <- as.dendrogram(hc_single_scaling_yes)
single_col_dend_scaled <- color_branches(single_dend_obj_scaled, k = 4)
plot(single_col_dend_scaled, main="Hierarchisches Clustern: Single-Linkage und skaliert ")
```

*5)* Linkage: Average | Scaling: Yes
```{r}
hc_average_scaling_yes <- hclust(dist_mat2, method = 'average')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
average_dend_obj_scaled <- as.dendrogram(hc_average_scaling_yes)
average_col_dend_scaled <- color_branches(average_dend_obj_scaled, k = 4)
plot(average_col_dend_scaled,main="Hierarchisches Clustern: Average-Linkage und skaliert")
```

*6)* Linkage: Complete | Scaling: Yes
```{r}
hc_complete_scaling_yes <- hclust(dist_mat2, method = 'complete')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
complete_dend_obj_scaled <- as.dendrogram(hc_complete_scaling_yes)
complete_col_dend_scaled <- color_branches(complete_dend_obj_scaled, k = 4)
plot(complete_col_dend_scaled,main="Hierarchisches Clustern: Complete-Linkage und skaliert ")
```

**(b)** Basierend auf den obigen Plots, erscheint eines der Ergebnisse nützlicher als die anderen? (Es gibt hier keine richtige Antwort.) Wählen Sie Ihren Favoriten. (Nochmals, keine richtige Antwort.)

Welcher Plot am sinnvollsten ist, hängt vom Anwendungsfall ab:
Complete-Linkage misst den maximalen Abstand vor dem clustern, der single-Linkage berechnet den minimalsten Abstand vor dem clustern. Beim Single-Linkage können so Outliers gesichtet werden. Welche die richtige Linkage-Methode ist, hängt vom Anwendungsfall ab, deshalb gibt es keine richtige oder falsche Methode. Ich würde hier die Complete-Linkage mit skalierten Daten bevorzugen, da diese die Daten am ehesten gut trennt.

**(c)** Verwenden Sie die Dokumentation zu `?hclust`, um weitere mögliche Verknüpfungen zu finden. Such dir einen aus und probiere ihn aus. Vergleichen Sie die Ergebnisse mit Ihren Favoriten von **(b)**. Ist es anders?

```{r}
hc_centroid_scaling_yes <- hclust(dist_mat2, method = 'centroid')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
centroid_dend_obj_scaled <- as.dendrogram(hc_centroid_scaling_yes)
centroid_col_dend_scaled <- color_branches(hc_centroid_scaling_yes, k = 4)
plot(centroid_col_dend_scaled,main="Centroid Clustern: Average-Linkage und skaliert")

```
Eine weitere Methode ist z.B.: die `centroid-linkage`: Sie sucht den Schwerpunkt von den jeweiligen Clustern (in unseren Beispiel 4) und berechnet die Distanz davon vor dem Zusammenführen.  Beim Plotten mit fällt sofort auf, dass die Ergebnisse nicht mehr `geordnet`sind, wie bei den obigen Plots, sondern graphish `wild durcheinander laufen`.

**(d)** Verwenden Sie die Dokumentation zu `?dist`, um andere mögliche Entfernungsmessungen zu finden. (Wir haben `euklidisch` verwendet.) Wählen Sie eine (nicht `binär`) und versuchen Sie es. Vergleichen Sie die Ergebnisse mit Ihren Favoriten von **(b)**. Ist es anders?
```{r}
dist_mat3 <- dist(df2, method = 'manhattan')
```

Linkage: Complete | Scaling: Yes
```{r}
hc_complete_scaling_yes_2 <- hclust(dist_mat3, method = 'complete')
```

```{r, echo=F}
graphics.off()
options(scipen=999)
```
Dendrogramm mit vier verschiedenen Clustern: 
```{r, echo=F}
complete_dend_obj_scaled <- as.dendrogram(hc_complete_scaling_yes_2)
complete_col_dend_scaled <- color_branches(complete_dend_obj_scaled, k = 4)
plot(complete_col_dend_scaled,main="Hierarchisches Clustern: Complete-Linkage, skaliert und Manhattan-Dist.")
```

Verglichen mit der Complete-Linkage, skaliert und euklidischer Distanz erhalten wir wieder neue vier Cluster mit unterschiedlichen Werten bei der Verwendung der Manhattan-Distanz.